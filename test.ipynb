{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "\n",
    "train, dev, test = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome Evaluation Metric for the task\n",
    "data = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer=Prediction(\n",
       "    rationale='produce the answer. We know that the context states that the sky is blue. Therefore, based on this information, we can conclude that the answer to the question \"Is the sky blue?\" is Yes.',\n",
       "    answer='Yes.'\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)\n",
    "data.keys()\n",
    "data['question'], data['answer']\n",
    "'question' in data\n",
    "\n",
    "# let the LLM do the prediction here\n",
    "\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=500)\n",
    "dspy.settings.configure(lm=turbo, trace=[], temperature=0.7)\n",
    "\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Provide an answer to a question.\"\"\"\n",
    "    context = dspy.InputField(desc=\"may contain relavent facts\")\n",
    "    question = dspy.InputField(desc=\"the question to be answered\")\n",
    "    answer = dspy.OutputField(desc=\"the answer to the question, should be either Yes or No\")\n",
    "\n",
    "class giveQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gen_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question, context):\n",
    "        pred = self.gen_answer(context=context, question=question)\n",
    "        pred = dspy.Prediction(answer = pred)\n",
    "        return pred\n",
    "\n",
    "giveQA = giveQA()\n",
    "giveQA('Is the sky blue?', 'The sky is blue.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How would a typical person answer each of the following questions about causation?\\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\\nOptions:\\n- Yes\\n- No'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
