Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 11 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4396 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4187 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4237 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4337 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4258 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4125 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4475 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4204 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4138 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4165 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4242 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4151 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4200 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4134 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4204 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4426 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4334 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, your messages resulted in 4226 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4285 tokens (3285 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4356 tokens (3356 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4435 tokens (3435 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4494 tokens (3494 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4302 tokens (3302 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4223 tokens (3223 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4340 tokens (3340 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4573 tokens (3573 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4302 tokens (3302 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4298 tokens (3298 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4263 tokens (3263 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4249 tokens (3249 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4335 tokens (3335 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4232 tokens (3232 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4324 tokens (3324 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4432 tokens (3432 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4524 tokens (3524 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4223 tokens (3223 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4494 tokens (3494 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4302 tokens (3302 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4285 tokens (3285 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4435 tokens (3435 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4356 tokens (3356 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4340 tokens (3340 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4263 tokens (3263 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4335 tokens (3335 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4573 tokens (3573 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4302 tokens (3302 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4232 tokens (3232 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4298 tokens (3298 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4249 tokens (3249 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4432 tokens (3432 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4324 tokens (3324 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4524 tokens (3524 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4435 tokens (3435 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4302 tokens (3302 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4356 tokens (3356 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4285 tokens (3285 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4302 tokens (3302 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4223 tokens (3223 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4263 tokens (3263 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4335 tokens (3335 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4573 tokens (3573 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4494 tokens (3494 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4298 tokens (3298 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4432 tokens (3432 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4324 tokens (3324 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4232 tokens (3232 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4340 tokens (3340 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4249 tokens (3249 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
error_code=context_length_exceeded error_message="This model's maximum context length is 4097 tokens. However, you requested 4524 tokens (3524 in the messages, 1000 in the completion). Please reduce the length of the messages or completion." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
